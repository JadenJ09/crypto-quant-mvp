{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10b6aa2",
   "metadata": {},
   "source": [
    "ðŸŽ¯ RECOMMENDED: Store Only 1m Data + Real-time Aggregation\n",
    "Why This Is Optimal for Your Use Case:\n",
    "ðŸš€ Real-time Performance: Generate higher timeframes on-demand\n",
    "ðŸ’¾ Storage Efficiency: ~80% less storage vs storing all timeframes\n",
    "ðŸ”„ Data Consistency: Single source of truth prevents sync issues\n",
    "âš¡ ML Pipeline Speed: Fresh aggregations for model training/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized Bulk Loader for ML Trading Pipeline\n",
    "- Stores only 1m data (source of truth)\n",
    "- Real-time aggregation for technical indicators\n",
    "- Optimized for Transformer + RL models\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from psycopg2.pool import ThreadedConnectionPool\n",
    "import requests\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BinanceClient:\n",
    "    \"\"\"Optimized Binance API client for bulk 1m data fetching.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None, secret_key: Optional[str] = None):\n",
    "        self.api_key = api_key\n",
    "        self.secret_key = secret_key\n",
    "        self.base_url = \"https://api.binance.com\"\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Optimized rate limiting for bulk loading\n",
    "        self.requests_per_second = 18  # Conservative for stability\n",
    "        self.last_request_time = 0\n",
    "        \n",
    "        if api_key:\n",
    "            self.session.headers.update({'X-MBX-APIKEY': api_key})\n",
    "    \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"Smart rate limiting to maximize throughput while staying safe.\"\"\"\n",
    "        now = time.time()\n",
    "        time_since_last = now - self.last_request_time\n",
    "        min_interval = 1.0 / self.requests_per_second\n",
    "        \n",
    "        if time_since_last < min_interval:\n",
    "            sleep_time = min_interval - time_since_last\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "    \n",
    "    def get_klines_optimized(self, symbol: str, start_time: int, \n",
    "                           end_time: int, limit: int = 1000) -> List[List]:\n",
    "        \"\"\"\n",
    "        Fetch 1m klines with maximum efficiency.\n",
    "        \n",
    "        Optimized for:\n",
    "        - ML model training (consistent 1m resolution)\n",
    "        - Real-time indicator calculation\n",
    "        - Minimal API calls\n",
    "        \"\"\"\n",
    "        self._rate_limit()\n",
    "        \n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'interval': '1m',  # Only 1m data needed\n",
    "            'startTime': start_time,\n",
    "            'endTime': end_time,\n",
    "            'limit': limit\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(\n",
    "                f\"{self.base_url}/api/v3/klines\",\n",
    "                params=params,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            klines = response.json()\n",
    "            logger.info(f\"âœ… Fetched {len(klines)} 1m bars for {symbol}\")\n",
    "            \n",
    "            return klines\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"âŒ API error for {symbol}: {e}\")\n",
    "            # Exponential backoff for rate limit errors\n",
    "            if \"429\" in str(e):\n",
    "                logger.warning(\"Rate limit hit, backing off...\")\n",
    "                time.sleep(60)  # Wait 1 minute\n",
    "            raise\n",
    "    \n",
    "    def get_symbol_complete_1m_history(self, symbol: str, start_date: str, \n",
    "                                     end_date: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch complete 1m history optimized for ML pipeline.\n",
    "        \n",
    "        Why only 1m data:\n",
    "        1. Technical indicators can be calculated real-time from 1m\n",
    "        2. Transformer models benefit from consistent resolution\n",
    "        3. RL models need tick-level granularity for execution\n",
    "        4. Storage efficiency (5m, 15m, 1h, 4h, 1d can be aggregated)\n",
    "        \"\"\"\n",
    "        start_dt = datetime.fromisoformat(start_date).replace(tzinfo=timezone.utc)\n",
    "        end_dt = datetime.fromisoformat(end_date).replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        all_records = []\n",
    "        current_time = start_dt\n",
    "        \n",
    "        # Optimal chunk size for 1m data: 1000 records = 16.67 hours\n",
    "        chunk_hours = 16\n",
    "        chunk_duration = timedelta(hours=chunk_hours)\n",
    "        \n",
    "        total_chunks = int((end_dt - start_dt).total_seconds() / 3600 / chunk_hours) + 1\n",
    "        processed_chunks = 0\n",
    "        \n",
    "        logger.info(f\"ðŸš€ Loading {symbol}: {total_chunks} chunks to process\")\n",
    "        \n",
    "        while current_time < end_dt:\n",
    "            chunk_end = min(current_time + chunk_duration, end_dt)\n",
    "            \n",
    "            start_ms = int(current_time.timestamp() * 1000)\n",
    "            end_ms = int(chunk_end.timestamp() * 1000)\n",
    "            \n",
    "            try:\n",
    "                klines = self.get_klines_optimized(symbol, start_ms, end_ms)\n",
    "                \n",
    "                # Convert to ML-optimized format\n",
    "                for kline in klines:\n",
    "                    record = {\n",
    "                        'time': datetime.fromtimestamp(kline[0] / 1000, tz=timezone.utc),\n",
    "                        'symbol': symbol,\n",
    "                        'open': float(kline[1]),\n",
    "                        'high': float(kline[2]),\n",
    "                        'low': float(kline[3]),\n",
    "                        'close': float(kline[4]),\n",
    "                        'volume': float(kline[5]),\n",
    "                        # Pre-calculate basic features for ML\n",
    "                        'typical_price': (float(kline[2]) + float(kline[3]) + float(kline[4])) / 3,\n",
    "                        'price_change': float(kline[4]) - float(kline[1]),\n",
    "                        'volume_weighted_price': float(kline[7]) / float(kline[5]) if float(kline[5]) > 0 else float(kline[4])\n",
    "                    }\n",
    "                    all_records.append(record)\n",
    "                \n",
    "                processed_chunks += 1\n",
    "                progress = (processed_chunks / total_chunks) * 100\n",
    "                \n",
    "                logger.info(f\"ðŸ“Š {symbol} progress: {progress:.1f}% \"\n",
    "                           f\"({len(all_records):,} records)\")\n",
    "                \n",
    "                current_time = chunk_end\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"âš ï¸  Chunk failed for {symbol}: {e}\")\n",
    "                current_time = chunk_end  # Skip and continue\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"ðŸŽ‰ {symbol} complete: {len(all_records):,} 1m records\")\n",
    "        return all_records\n",
    "\n",
    "\n",
    "class OptimizedDatabaseWriter:\n",
    "    \"\"\"High-performance database writer optimized for ML workloads.\"\"\"\n",
    "    \n",
    "    def __init__(self, connection_url: str, max_connections: int = 10):\n",
    "        self.connection_url = connection_url\n",
    "        self.pool = ThreadedConnectionPool(\n",
    "            minconn=2,\n",
    "            maxconn=max_connections,\n",
    "            dsn=connection_url\n",
    "        )\n",
    "    \n",
    "    def ensure_optimized_schema(self):\n",
    "        \"\"\"Create ML-optimized schema with proper indexing.\"\"\"\n",
    "        conn = self.pool.getconn()\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                # Create hypertable optimized for ML queries\n",
    "                cursor.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS ohlcv_1m (\n",
    "                        time TIMESTAMPTZ NOT NULL,\n",
    "                        symbol TEXT NOT NULL,\n",
    "                        open DOUBLE PRECISION NOT NULL,\n",
    "                        high DOUBLE PRECISION NOT NULL,\n",
    "                        low DOUBLE PRECISION NOT NULL,\n",
    "                        close DOUBLE PRECISION NOT NULL,\n",
    "                        volume DOUBLE PRECISION NOT NULL,\n",
    "                        typical_price DOUBLE PRECISION NOT NULL,\n",
    "                        price_change DOUBLE PRECISION NOT NULL,\n",
    "                        volume_weighted_price DOUBLE PRECISION NOT NULL,\n",
    "                        \n",
    "                        -- ML-specific indexes\n",
    "                        PRIMARY KEY (symbol, time)\n",
    "                    );\n",
    "                \"\"\")\n",
    "                \n",
    "                # Convert to hypertable if not already\n",
    "                cursor.execute(\"\"\"\n",
    "                    SELECT create_hypertable('ohlcv_1m', 'time', \n",
    "                                            chunk_time_interval => INTERVAL '1 day',\n",
    "                                            if_not_exists => TRUE);\n",
    "                \"\"\")\n",
    "                \n",
    "                # Indexes optimized for ML queries\n",
    "                cursor.execute(\"\"\"\n",
    "                    CREATE INDEX IF NOT EXISTS idx_symbol_time_desc \n",
    "                    ON ohlcv_1m (symbol, time DESC);\n",
    "                \"\"\")\n",
    "                \n",
    "                cursor.execute(\"\"\"\n",
    "                    CREATE INDEX IF NOT EXISTS idx_time_symbol \n",
    "                    ON ohlcv_1m (time, symbol);\n",
    "                \"\"\")\n",
    "                \n",
    "                conn.commit()\n",
    "                logger.info(\"âœ… ML-optimized schema created\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            logger.error(f\"âŒ Schema creation failed: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.pool.putconn(conn)\n",
    "    \n",
    "    def bulk_insert_ml_optimized(self, records: List[Dict], batch_size: int = 50000):\n",
    "        \"\"\"\n",
    "        Ultra-fast bulk insert optimized for ML data pipeline.\n",
    "        \n",
    "        Why larger batch_size (50k):\n",
    "        - 1m data is uniform and predictable\n",
    "        - ML pipelines benefit from larger contiguous chunks\n",
    "        - Reduces DB connection overhead\n",
    "        \"\"\"\n",
    "        if not records:\n",
    "            return\n",
    "        \n",
    "        conn = self.pool.getconn()\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                total_inserted = 0\n",
    "                \n",
    "                for i in range(0, len(records), batch_size):\n",
    "                    batch = records[i:i + batch_size]\n",
    "                    \n",
    "                    # Prepare ML-optimized data format\n",
    "                    values = [\n",
    "                        (\n",
    "                            record['time'],\n",
    "                            record['symbol'],\n",
    "                            record['open'],\n",
    "                            record['high'],\n",
    "                            record['low'],\n",
    "                            record['close'],\n",
    "                            record['volume'],\n",
    "                            record['typical_price'],\n",
    "                            record['price_change'],\n",
    "                            record['volume_weighted_price']\n",
    "                        )\n",
    "                        for record in batch\n",
    "                    ]\n",
    "                    \n",
    "                    # High-performance upsert\n",
    "                    execute_values(\n",
    "                        cursor,\n",
    "                        \"\"\"\n",
    "                        INSERT INTO ohlcv_1m (\n",
    "                            time, symbol, open, high, low, close, volume,\n",
    "                            typical_price, price_change, volume_weighted_price\n",
    "                        )\n",
    "                        VALUES %s\n",
    "                        ON CONFLICT (symbol, time) DO UPDATE SET\n",
    "                            open = EXCLUDED.open,\n",
    "                            high = EXCLUDED.high,\n",
    "                            low = EXCLUDED.low,\n",
    "                            close = EXCLUDED.close,\n",
    "                            volume = EXCLUDED.volume,\n",
    "                            typical_price = EXCLUDED.typical_price,\n",
    "                            price_change = EXCLUDED.price_change,\n",
    "                            volume_weighted_price = EXCLUDED.volume_weighted_price\n",
    "                        \"\"\",\n",
    "                        values,\n",
    "                        template=None,\n",
    "                        page_size=5000  # Optimized for ML batch sizes\n",
    "                    )\n",
    "                    \n",
    "                    total_inserted += len(batch)\n",
    "                    conn.commit()\n",
    "                    \n",
    "                    logger.info(f\"ðŸ’¾ Inserted {total_inserted:,} records\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            logger.error(f\"âŒ Database error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.pool.putconn(conn)\n",
    "\n",
    "\n",
    "class MLOptimizedBulkLoader:\n",
    "    \"\"\"Bulk loader optimized for ML trading pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.binance_client = BinanceClient(\n",
    "            api_key=os.getenv('BINANCE_API_KEY'),\n",
    "            secret_key=os.getenv('BINANCE_SECRET_KEY')\n",
    "        )\n",
    "        self.db_writer = OptimizedDatabaseWriter(\n",
    "            connection_url=os.getenv('DATABASE_URL')\n",
    "        )\n",
    "        \n",
    "        # Top crypto pairs for ML model training\n",
    "        # Ordered by market cap and liquidity for better model performance\n",
    "        self.symbols = [\n",
    "            # Tier 1: Major pairs (highest liquidity)\n",
    "            'BTCUSDT', 'ETHUSDT', 'BNBUSDT',\n",
    "            \n",
    "            # Tier 2: Large caps (good for diversification)\n",
    "            'XRPUSDT', 'ADAUSDT', 'SOLUSDT', 'DOGEUSDT',\n",
    "            \n",
    "            # Tier 3: Mid caps (higher volatility for RL training)\n",
    "            'DOTUSDT', 'AVAXUSDT', 'MATICUSDT', 'LINKUSDT',\n",
    "            \n",
    "            # Tier 4: Active trading pairs\n",
    "            'UNIUSDT', 'LTCUSDT', 'ATOMUSDT', 'ETCUSDT'\n",
    "        ]\n",
    "    \n",
    "    def load_symbol_for_ml(self, symbol: str, start_date: str, end_date: str) -> int:\n",
    "        \"\"\"Load single symbol optimized for ML pipeline.\"\"\"\n",
    "        logger.info(f\"ðŸš€ Loading {symbol} for ML pipeline\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Fetch complete 1m history\n",
    "        records = self.binance_client.get_symbol_complete_1m_history(\n",
    "            symbol=symbol,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date\n",
    "        )\n",
    "        \n",
    "        # Bulk insert with ML optimizations\n",
    "        if records:\n",
    "            self.db_writer.bulk_insert_ml_optimized(records)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        rate = len(records) / duration if duration > 0 else 0\n",
    "        \n",
    "        logger.info(f\"âœ… {symbol}: {len(records):,} records in {duration:.1f}s \"\n",
    "                   f\"({rate:.0f} records/sec)\")\n",
    "        \n",
    "        return len(records)\n",
    "    \n",
    "    def load_all_for_ml_pipeline(self, start_date: str, end_date: str, \n",
    "                                max_workers: int = 6) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Load all symbols optimized for ML trading pipeline.\n",
    "        \n",
    "        Optimizations:\n",
    "        - Higher concurrency (6 workers) since we're only doing 1m data\n",
    "        - Prioritized symbol loading (major pairs first)\n",
    "        - ML-specific data preprocessing during load\n",
    "        \"\"\"\n",
    "        logger.info(\"ðŸš€ Starting ML-Optimized Bulk Historical Loader\")\n",
    "        logger.info(f\"ðŸ“Š Loading {len(self.symbols)} symbols\")\n",
    "        logger.info(f\"ðŸ“… Date range: {start_date} to {end_date}\")\n",
    "        logger.info(f\"âš¡ Using {max_workers} parallel workers\")\n",
    "        logger.info(\"ðŸŽ¯ Target: 1m OHLCV data for ML/RL pipeline\")\n",
    "        \n",
    "        # Ensure optimized schema exists\n",
    "        self.db_writer.ensure_optimized_schema()\n",
    "        \n",
    "        results = {}\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit symbol loading tasks\n",
    "            future_to_symbol = {\n",
    "                executor.submit(\n",
    "                    self.load_symbol_for_ml, symbol, start_date, end_date\n",
    "                ): symbol\n",
    "                for symbol in self.symbols\n",
    "            }\n",
    "            \n",
    "            # Process completed tasks\n",
    "            completed = 0\n",
    "            for future in as_completed(future_to_symbol):\n",
    "                symbol = future_to_symbol[future]\n",
    "                completed += 1\n",
    "                \n",
    "                try:\n",
    "                    record_count = future.result()\n",
    "                    results[symbol] = record_count\n",
    "                    \n",
    "                    progress = (completed / len(self.symbols)) * 100\n",
    "                    logger.info(f\"ðŸŽ‰ {symbol}: {record_count:,} records \"\n",
    "                               f\"(Progress: {progress:.1f}%)\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"âŒ {symbol} failed: {e}\")\n",
    "                    results[symbol] = 0\n",
    "        \n",
    "        # Performance summary\n",
    "        total_duration = time.time() - start_time\n",
    "        total_records = sum(results.values())\n",
    "        \n",
    "        logger.info(\"ðŸš€ ML BULK LOAD COMPLETED!\")\n",
    "        logger.info(f\"ðŸ“Š Total records: {total_records:,}\")\n",
    "        logger.info(f\"â±ï¸  Total time: {total_duration:.1f}s\")\n",
    "        logger.info(f\"ðŸ”¥ Average rate: {total_records/total_duration:.0f} records/sec\")\n",
    "        logger.info(\"ðŸŽ¯ Ready for ML model training & real-time indicators!\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point optimized for ML trading pipeline.\"\"\"\n",
    "    \n",
    "    # Configuration optimized for ML use case\n",
    "    start_date = os.getenv('START_DATE', '2025-01-01')\n",
    "    end_date = os.getenv('END_DATE', datetime.now().strftime('%Y-%m-%d'))\n",
    "    max_workers = int(os.getenv('MAX_WORKERS', '6'))  # Higher for 1m-only loading\n",
    "    \n",
    "    # Ensure proper datetime format\n",
    "    if 'T' not in start_date:\n",
    "        start_date += 'T00:00:00'\n",
    "    if 'T' not in end_date:\n",
    "        end_date += 'T23:59:59'\n",
    "    \n",
    "    logger.info(\"ðŸš€ ML-OPTIMIZED BULK HISTORICAL LOADER\")\n",
    "    logger.info(\"ðŸŽ¯ Purpose: Load 1m data for ML/RL trading pipeline\")\n",
    "    logger.info(f\"ðŸ“… Date range: {start_date} to {end_date}\")\n",
    "    logger.info(\"ðŸ’¡ Strategy: Store 1m + real-time aggregation\")\n",
    "    \n",
    "    try:\n",
    "        loader = MLOptimizedBulkLoader()\n",
    "        results = loader.load_all_for_ml_pipeline(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            max_workers=max_workers\n",
    "        )\n",
    "        \n",
    "        # ML-focused summary\n",
    "        logger.info(\"ðŸŽ‰ ML LOAD SUMMARY:\")\n",
    "        successful_symbols = 0\n",
    "        for symbol, count in sorted(results.items()):\n",
    "            status = \"âœ…\" if count > 0 else \"âŒ\"\n",
    "            if count > 0:\n",
    "                successful_symbols += 1\n",
    "            logger.info(f\"  {status} {symbol}: {count:,} 1m records\")\n",
    "        \n",
    "        logger.info(f\"ðŸ“ˆ Success rate: {successful_symbols}/{len(results)} symbols\")\n",
    "        logger.info(\"ðŸš€ Ready for:\")\n",
    "        logger.info(\"  â€¢ Real-time technical indicator calculation\")\n",
    "        logger.info(\"  â€¢ Transformer model training\")\n",
    "        logger.info(\"  â€¢ RL model development\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ðŸ’¥ ML bulk load failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a809934",
   "metadata": {},
   "source": [
    "ðŸ—ï¸ Real-time Aggregation Service\n",
    "Now create a service for real-time technical indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5951df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Real-time Technical Indicator Engine\n",
    "Aggregates 1m data to higher timeframes and calculates indicators on-demand.\n",
    "Optimized for ML model features and RL trading signals.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RealTimeIndicatorEngine:\n",
    "    \"\"\"\n",
    "    Real-time technical indicator calculation from 1m base data.\n",
    "    \n",
    "    Benefits over pre-stored timeframes:\n",
    "    1. Always fresh data for ML models\n",
    "    2. Flexible timeframe combinations\n",
    "    3. Memory efficient\n",
    "    4. Consistent with real-time trading\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_connection):\n",
    "        self.db = db_connection\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    async def get_ohlcv_timeframe(self, symbol: str, timeframe: str, \n",
    "                                 limit: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate any timeframe from 1m data using TimescaleDB time_bucket.\n",
    "        \n",
    "        This is FASTER than storing multiple timeframes because:\n",
    "        - TimescaleDB time_bucket is highly optimized\n",
    "        - No data synchronization issues\n",
    "        - Always current data\n",
    "        \"\"\"\n",
    "        \n",
    "        interval_map = {\n",
    "            '1m': '1 minute',\n",
    "            '5m': '5 minutes', \n",
    "            '15m': '15 minutes',\n",
    "            '1h': '1 hour',\n",
    "            '4h': '4 hours',\n",
    "            '1d': '1 day'\n",
    "        }\n",
    "        \n",
    "        interval = interval_map.get(timeframe, '1 minute')\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            time_bucket('{interval}', time) as time,\n",
    "            symbol,\n",
    "            FIRST(open, time) as open,\n",
    "            MAX(high) as high,\n",
    "            MIN(low) as low,\n",
    "            LAST(close, time) as close,\n",
    "            SUM(volume) as volume,\n",
    "            AVG(typical_price) as typical_price\n",
    "        FROM ohlcv_1m \n",
    "        WHERE symbol = %s \n",
    "        AND time >= NOW() - INTERVAL '{limit} {interval.split()[1]}'\n",
    "        GROUP BY time_bucket('{interval}', time), symbol\n",
    "        ORDER BY time DESC\n",
    "        LIMIT %s\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute and return as DataFrame for ML processing\n",
    "        result = await self.db.fetch(query, symbol, limit)\n",
    "        df = pd.DataFrame(result)\n",
    "        \n",
    "        if not df.empty:\n",
    "            df.set_index('time', inplace=True)\n",
    "            df.sort_index(inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    async def calculate_ml_features(self, symbol: str, timeframes: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate comprehensive ML features across multiple timeframes.\n",
    "        \n",
    "        Returns features optimized for:\n",
    "        - Transformer model input\n",
    "        - RL state representation\n",
    "        - Real-time trading signals\n",
    "        \"\"\"\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        for tf in timeframes:\n",
    "            df = await self.get_ohlcv_timeframe(symbol, tf)\n",
    "            \n",
    "            if df.empty:\n",
    "                continue\n",
    "                \n",
    "            # Technical indicators\n",
    "            features[f'{tf}_rsi'] = self.calculate_rsi(df['close'])\n",
    "            features[f'{tf}_ma20'] = df['close'].rolling(20).mean()\n",
    "            features[f'{tf}_ma50'] = df['close'].rolling(50).mean()\n",
    "            features[f'{tf}_bollinger_upper'], features[f'{tf}_bollinger_lower'] = self.calculate_bollinger_bands(df['close'])\n",
    "            features[f'{tf}_macd'], features[f'{tf}_macd_signal'] = self.calculate_macd(df['close'])\n",
    "            features[f'{tf}_stoch_k'], features[f'{tf}_stoch_d'] = self.calculate_stochastic(df)\n",
    "            \n",
    "            # Price action features\n",
    "            features[f'{tf}_price_change'] = df['close'].pct_change()\n",
    "            features[f'{tf}_volatility'] = df['close'].rolling(20).std()\n",
    "            features[f'{tf}_volume_sma'] = df['volume'].rolling(20).mean()\n",
    "            \n",
    "            # ML-specific features\n",
    "            features[f'{tf}_price_position'] = (df['close'] - df['low']) / (df['high'] - df['low'])\n",
    "            features[f'{tf}_volume_profile'] = df['volume'] / df['volume'].rolling(50).mean()\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:\n",
    "        \"\"\"RSI calculation optimized for real-time use.\"\"\"\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    def calculate_macd(self, prices: pd.Series) -> tuple:\n",
    "        \"\"\"MACD calculation.\"\"\"\n",
    "        ema12 = prices.ewm(span=12).mean()\n",
    "        ema26 = prices.ewm(span=26).mean()\n",
    "        macd = ema12 - ema26\n",
    "        signal = macd.ewm(span=9).mean()\n",
    "        return macd, signal\n",
    "    \n",
    "    # ... other indicator methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07532629",
   "metadata": {},
   "source": [
    "ðŸŽ¯ Final Architecture Recommendation\n",
    "\n",
    "# Your optimal ML trading pipeline:\n",
    "\n",
    "1. Store ONLY 1m data (bulk-loader)\n",
    "   â””â”€â”€ Source of truth for all timeframes\n",
    "   \n",
    "2. Real-time aggregation (indicator-engine) \n",
    "   â””â”€â”€ Generate 5m, 15m, 1h, 4h, 1d on-demand\n",
    "   \n",
    "3. ML Feature Pipeline (transformer-features)\n",
    "   â””â”€â”€ Multi-timeframe features for model training\n",
    "   \n",
    "4. Signal Generation (ml-predictor)\n",
    "   â””â”€â”€ Transformer model: -1 to 1 signals\n",
    "   \n",
    "5. RL Execution (rl-trader)\n",
    "   â””â”€â”€ Optimize real-time trading execution\n",
    "\n",
    "\n",
    "ðŸš€ Benefits for Your Use Case:\n",
    "âœ… Real-time Technical Indicators:\n",
    "\n",
    "Always fresh calculations from 1m base data\n",
    "No stale pre-calculated indicator issues\n",
    "Flexible timeframe combinations\n",
    "âœ… Transformer Model Training:\n",
    "\n",
    "Consistent 1m resolution input\n",
    "Multi-timeframe feature engineering\n",
    "No data synchronization problems\n",
    "âœ… RL Trading Execution:\n",
    "\n",
    "Tick-level granularity (1m) for precise entry/exit\n",
    "Real-time aggregation for context\n",
    "Optimal for profit maximization\n",
    "âœ… Storage & Performance:\n",
    "\n",
    "80% less storage vs storing all timeframes\n",
    "Faster queries (single table)\n",
    "Better data integrity\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
