# docker-compose.dev.yml
# This file defines and configures all the services needed to run the Crypto-Quant-MVP
# backend locally for development. Run with: docker compose -f docker-compose.dev.yml up --build -d\
# UPDATED for P1-T3 to include a local Spark cluster.

networks:
  # A dedicated network for our services to communicate reliably using their names.
  quant_network:
    driver: bridge

services:
  # --------------------------------------------------------------------------
  #  DATA & MESSAGING TIER
  # --------------------------------------------------------------------------

  # Kafka is our central message bus, the "data highway" of the system.
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    hostname: kafka
    container_name: kafka
    networks:
      - quant_network
    ports:
      # Exposes Kafka to the host machine for local tools or testing.
      - "9092:9092"
    volumes:
      # Persist Kafka logs and data
      - kafka_data:/var/lib/kafka/data
    environment:
      # --- KRaft Mode Configuration (No Zookeeper Needed) ---
      # Uniquely identifies this node in the cluster.
      KAFKA_NODE_ID: 1
      # Defines the roles of this node (broker for data, controller for metadata).
      KAFKA_PROCESS_ROLES: "broker,controller"
      # Lists the initial voting members of the metadata quorum. For a single node, it's just itself.
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      # Specifies which listener the controller should use.
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      # --- Listener Configuration (Crucial for Connectivity) ---
      # Defines how different clients connect. PLAINTEXT is for internal Docker network comms.
      # PLAINTEXT_HOST is for external comms from your host machine (e.g., a local Python script).
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
      KAFKA_LISTENERS: "PLAINTEXT://:29092,PLAINTEXT_HOST://:9092,CONTROLLER://:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092"
      # Defines which listener other brokers in a cluster would use.
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      # --- Topic and Cluster Configuration ---
      # A unique ID for the Kafka cluster. Generate one for your project.
      CLUSTER_ID: "PWK0PmUZSUSwsY00holpVA" # UUID encoded in base64 needed for KRaft mode. -> python3 -c 'import uuid, base64; print(base64.urlsafe_b64encode(uuid.uuid4().bytes).decode("ascii").rstrip("="))'
      # For a single-node setup, these factors must be 1.
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      # Allows topics to be created automatically when a producer sends a message to them.
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "kafka-topics --bootstrap-server kafka:29092 --list || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s # Give Kafka some time to start up before starting health checks

  # TimescaleDB is our high-performance "hot" database for time-series and vector data.
  timescaledb:
    build:
      context: ./services/timescaledb
      dockerfile: Dockerfile
    image: crypto_quant_mvp/timescaledb-custom # Optional: name the custom image
    hostname: timescaledb
    container_name: timescaledb
    networks:
      - quant_network
    ports:
      # Exposes the PostgreSQL port to the host machine for DB tools like DBeaver or pgAdmin.
      - "5433:5433" # Both external and internal port are 5433
    volumes:
      # Persists all database data on the host machine. If you stop/remove the container,
      # your data will be safe and will be re-loaded on the next start.
      - timescale_data:/var/lib/postgresql/data
      - ./infra/db:/docker-entrypoint-initdb.d # Custom initialization scripts
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      PGPORT: 5433 # Set PostgreSQL to listen on port 5433 internally
    # --- healthcheck block ---
    healthcheck:
      test:
        ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB -p 5433"]
      interval: 5s
      timeout: 5s
      retries: 5
    # -----------------------------------------------

  # --------------------------------------------------------------------------
  #  PROCESSING TIER (NEWLY ADDED: P1-T3)
  # --------------------------------------------------------------------------
  spark-master:
    image: bitnami/spark:3.5.1
    user: root
    container_name: spark-master
    networks:
      - quant_network
    ports:
      - "8080:8080" # Spark Master Web UI
      - "7077:7077" # Spark Master Port for job submission
    volumes:
      # Mount our spark-jobs directory so the master can access the scripts
      - ./spark-jobs:/opt/bitnami/spark/jobs
      # Mount a local directory to act as our local Delta Lake and checkpoint location
      - ./data:/opt/bitnami/spark/data
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no

  spark-worker:
    image: bitnami/spark:3.5.1
    user: root
    container_name: spark-worker
    networks:
      - quant_network
    depends_on:
      - spark-master
    volumes:
      - ./spark-jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no

    # Spark 잡을 자동으로 제출해주는 서비스
  spark-job-submitter:
    image: bitnami/spark:3.5.1
    user: root
    container_name: spark-job-submitter
    networks:
      - quant_network
    depends_on:
      - spark-master
      - spark-worker
    # spark-jobs 디렉토리를 컨테이너에 마운트합니다.
    volumes:
      - ./spark-jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data # spark-job이 data 폴더에 접근해야 할 경우를 대비
    # 이 서비스가 실행할 명령어입니다.
    command:
      - /bin/bash
      - -c
      - |
        chmod +x /opt/bitnami/spark/jobs/ohlcv_aggregator.py
        /opt/bitnami/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.postgresql:postgresql:42.7.3 \
          /opt/bitnami/spark/jobs/ohlcv_aggregator.py
    # 잡 제출 후 컨테이너가 재시작되지 않도록 설정합니다.
    restart: "no"

  # --------------------------------------------------------------------------
  #  APPLICATION TIER (Placeholders for now, to be built in next tasks)
  # --------------------------------------------------------------------------

  # --- NEW SERVICE ADDED FOR P1-T4 ---
  db-writer:
    build:
      context: ./services/db-writer
      dockerfile: Dockerfile
    container_name: db-writer
    networks:
      - quant_network
    depends_on:
      kafka:
        condition: service_started
      timescaledb:
        # Now depends_on will wait for the healthcheck to pass
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL} # .env 파일의 DATABASE_URL 값을 사용합니다.
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092"
      KAFKA_TOPIC: "agg.ohlcv.1m"
      KAFKA_GROUP_ID: "ohlcv_db_writer_group"
      # POSTGRES_HOST: "timescaledb"
      # POSTGRES_PORT: 5432
      # POSTGRES_USER: ${POSTGRES_USER}
      # POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      # POSTGRES_DB: ${POSTGRES_DB}
      # KAFKA_CONSUMER_GROUP_ID: "db-writer-group"
    restart: unless-stopped

  # The data ingestor service that will connect to Binance WebSockets.
  ingestor:
    # We will create the Dockerfile for this in task P1-T2.
    build:
      context: ./services/ingestor
      dockerfile: Dockerfile
    container_name: ingestor
    networks:
      - quant_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
      - 1.1.1.1
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092" # "29092" is the internal Docker network port.
      BINANCE_API_KEY: ${BINANCE_API_KEY}
      BINANCE_SECRET_KEY: ${BINANCE_SECRET_KEY}
    # The 'restart' policy ensures it automatically comes back up if it crashes.
    restart: unless-stopped

  # Data Recovery Service - Detects gaps and backfills missing OHLCV data
  data-recovery:
    build:
      context: ./services/data-recovery
      dockerfile: Dockerfile
    container_name: data-recovery
    networks:
      - quant_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
      - 1.1.1.1
    depends_on:
      kafka:
        condition: service_healthy
      timescaledb:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092"
      KAFKA_OUTPUT_TOPIC: "agg.ohlcv.1m"
      RECOVERY_MODE: "continuous" # or "oneshot"
      CHECK_INTERVAL_MINUTES: "10"
      HOURS_BACK: ${HOURS_BACK:-72} # Check last 3 days for more comprehensive monitoring
      START_DATE: ${START_DATE:-2025-01-01} # Gap detection start date
      END_DATE: ${END_DATE:-2025-06-28} # Gap detection end date
      BINANCE_API_KEY: ${BINANCE_API_KEY}
      BINANCE_SECRET_KEY: ${BINANCE_SECRET_KEY}
    restart: unless-stopped
    profiles:
      - realtime

  # Gap Recovery Service - One-time gap detection and backfilling
  gap-recovery:
    build:
      context: ./services/data-recovery
      dockerfile: Dockerfile
    container_name: gap-recovery
    networks:
      - quant_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
      - 1.1.1.1
    depends_on:
      kafka:
        condition: service_healthy
      timescaledb:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092"
      KAFKA_OUTPUT_TOPIC: "agg.ohlcv.1m"
      RECOVERY_MODE: "oneshot"
      HOURS_BACK: ${HOURS_BACK:-168} # Check last 7 days for one-time recovery
      START_DATE: ${START_DATE:-2025-01-01} # Gap detection start date
      END_DATE: ${END_DATE:-2025-06-28} # Gap detection end date
      BINANCE_API_KEY: ${BINANCE_API_KEY}
      BINANCE_SECRET_KEY: ${BINANCE_SECRET_KEY}
    restart: "no"
    profiles:
      - recovery

  # Hybrid Historical Data Collection Service - Intelligent gap detection and filling
  historical-collector:
    build:
      context: ./services/historical-collector
      dockerfile: Dockerfile
    container_name: historical-collector
    networks:
      - quant_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
      - 1.1.1.1
    depends_on:
      kafka:
        condition: service_healthy
      timescaledb:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092"
      KAFKA_OUTPUT_TOPIC: "agg.ohlcv.1m"
      START_DATE: ${START_DATE:-2025-01-01} # Gap detection start date
      END_DATE: ${END_DATE:-2025-06-28} # Gap detection end date
      MAX_WORKERS: ${MAX_WORKERS:-4} # Parallel symbol processing
      BINANCE_API_KEY: ${BINANCE_API_KEY}
      BINANCE_SECRET_KEY: ${BINANCE_SECRET_KEY}
    restart: "no"
    profiles:
      - historical

  # High-Performance Bulk Historical Data Loader - Direct API to DB for maximum speed
  bulk-loader:
    build:
      context: ./services/bulk-loader
      dockerfile: Dockerfile
    container_name: bulk-loader
    networks:
      - quant_network
    dns:
      - 8.8.8.8
      - 8.8.4.4
      - 1.1.1.1
    depends_on:
      timescaledb:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL}
      BINANCE_API_KEY: ${BINANCE_API_KEY}
      BINANCE_SECRET_KEY: ${BINANCE_SECRET_KEY}
      START_DATE: ${START_DATE:-2025-06-01}
      END_DATE: ${END_DATE:-2025-06-28}
      MAX_WORKERS: ${MAX_WORKERS:-4}
    restart: "no"
    profiles:
      - bulk-load

  # The main backend API that will serve data to the frontend.
  api:
    # We will create the Dockerfile and app for this in task P1-T5.
    build:
      context: ./services/api
      dockerfile: Dockerfile
    container_name: api
    networks:
      - quant_network
    ports:
      - "8000:8000"
    volumes:
      # Live-reloading: Any changes you make to the 'app' code on your host machine
      # are immediately reflected inside the running container.
      - ./services/api/app:/app/app
    depends_on:
      - timescaledb
      - kafka
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
    restart: unless-stopped

  # Frontend service
  frontend:
    build:
      context: ./services/frontend
      dockerfile: Dockerfile
    image: crypto_quant_mvp/frontend
    hostname: frontend
    container_name: frontend
    networks:
      - quant_network
    ports:
      - "3000:80"
    depends_on:
      - api
    restart: unless-stopped

  # --------------------------------------------------------------------------
  #  VECTORBT ANALYTICS TIER - Technical Indicators & Analytics
  # --------------------------------------------------------------------------

  # Bulk Historical Processing - One-shot operation to process all historical data
  vectorbt-bulk:
    build:
      context: ./services/vectorbt
      dockerfile: Dockerfile
    container_name: vectorbt-bulk
    networks:
      - quant_network
    depends_on:
      timescaledb:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL}
      SERVICE_MODE: "bulk" # Bulk historical processing mode
      LOG_LEVEL: "INFO"
      # Override defaults only if needed via .env file
      # BATCH_SIZE: ${BATCH_SIZE:-1000}
      # MAX_WORKERS: ${MAX_WORKERS:-4}
      # RSI_PERIODS: ${RSI_PERIODS:-"14,21,30"}
      # EMA_PERIODS: ${EMA_PERIODS:-"9,21,50,100,200"}
      # etc... (all other indicator settings use config.py defaults)
    restart: "no" # One-shot operation - don't restart
    profiles:
      - bulk-indicators

  # Real-time Indicators Processing - Continuous service for incremental updates
  vectorbt:
    build:
      context: ./services/vectorbt
      dockerfile: Dockerfile
    container_name: vectorbt
    networks:
      - quant_network
    depends_on:
      timescaledb:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DATABASE_URL}
      SERVICE_MODE: "indicators" # Real-time processing mode
      LOG_LEVEL: "INFO"
      POLLING_INTERVAL: "30" # Check for new data every 30 seconds
      # Override defaults only if needed via .env file
      # BATCH_SIZE: ${BATCH_SIZE:-1000}
      # MAX_WORKERS: ${MAX_WORKERS:-4}
      # RSI_PERIODS: ${RSI_PERIODS:-"14,21,30"}
      # EMA_PERIODS: ${EMA_PERIODS:-"9,21,50,100,200"}
      # etc... (all other indicator settings use config.py defaults)
    restart: unless-stopped
    profiles:
      - realtime

# Named volumes provide persistent storage for our stateful services.
volumes:
  timescale_data:
    driver: local
    # Optional: specify a custom location on host for backup purposes
    # driver_opts:
    #   o: bind
    #   type: none
    #   device: /path/to/your/data/directory

  # Volume for Kafka data persistence (optional but recommended)
  kafka_data:
    driver: local

  # Volume for Spark checkpoints and Delta Lake data
  spark_data:
    driver: local
